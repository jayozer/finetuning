{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6785f47a-5670-45a9-80df-a4d6f47b1181",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To complete the following guide you will need to install the following packages:\n",
    "- fireworks-ai\n",
    "- pandas\n",
    "- requests\n",
    "\n",
    "You will also need:\n",
    "\n",
    "- Fireworks account (https://fireworks.ai/)\n",
    "- Fireworks API key\n",
    "- The firectl command-line interface (https://docs.fireworks.ai/tools-sdks/firectl/firectl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e869493a-09c8-4036-8573-12a1d33e8723",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas requests fireworks-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b56a030b-9e82-43cc-8cde-ab20da4280a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "from fireworks.client import Fireworks\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b2526b-5718-4c9e-9e56-69314314d76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sign-in to your Fireworks account\n",
    "!firectl signin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a5636ee-8032-4f24-b69a-c90414078d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have the FIREWORKS_API_KEY environment variable set to your account's key!\n",
    "# os.environ['FIREWORKS_API_KEY'] = 'XXX'\n",
    "\n",
    "client = Fireworks()\n",
    "\n",
    "# Replace the line below with your Fireworks account id\n",
    "account_id = 'XXX'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c665b4ed-3a6d-4b22-a213-f286435eee1a",
   "metadata": {},
   "source": [
    "## Problem Definition: Insurance Support Ticket Classifier\n",
    "\n",
    "*Note: The problem definition, data, and labels used in this example were synthetically generated by Claude 3 Opus*\n",
    "\n",
    "In the insurance industry, customer support plays a crucial role in ensuring client satisfaction and retention. Insurance companies receive a high volume of support tickets daily, covering a wide range of topics such as billing, policy administration, claims assistance, and more. Manually categorizing these tickets can be time-consuming and inefficient, leading to longer response times and potentially impacting customer experience.\n",
    "\n",
    "### Task\n",
    "In last week's exercise, we performed prompt engineering to increase the accuracy of our predictions on the test.tsv dataset to 63.24%.\n",
    "\n",
    "This week, we will now fine-tune our first model to increase the accuracy even higher!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46da13a3-2659-4711-ab5e-2042895f2d45",
   "metadata": {},
   "source": [
    "#### Labeled Data\n",
    "\n",
    "The data can be found in the week-2 `data` folder.\n",
    "\n",
    "We will use the following datasets:\n",
    "- `./data/train.tsv`\n",
    "- `./data/test.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d53b80e9-1bf8-4da1-832e-6c44b781112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples = pd.read_csv('data/train.tsv', sep='\\t')\n",
    "test_examples = pd.read_csv('data/test.tsv', sep='\\t')\n",
    "\n",
    "# In order to not leak information about the test labels into our prompts, the list of possible categories will be defined \n",
    "# based on the training labels.\n",
    "categories = sorted(training_examples['label'].unique().tolist())\n",
    "categories_str = '\\n'.join(categories)\n",
    "\n",
    "training_tickets = training_examples['text'].tolist()\n",
    "training_labels = training_examples['label'].tolist()\n",
    "\n",
    "test_tickets = test_examples['text'].tolist()\n",
    "test_labels = test_examples['label'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810815db-a6b2-4d1c-bea8-f4975cbcc3b9",
   "metadata": {},
   "source": [
    "### Dataset Curation\n",
    "\n",
    "We first must transform our dataset into the format expected by Fireworks, and then upload the dataset. The dataset must conform to the schema expected by the Chat Completions API.\n",
    "\n",
    "See https://docs.fireworks.ai/fine-tuning/fine-tuning-models#conversation for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0f1f8c6a-90e7-4d32-b9b6-9de0a52c51f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(ticket):\n",
    "    return f\"\"\"classify a customer support ticket into one of the following categories:\n",
    "<categories>\n",
    "{categories_str}\n",
    "</categories>\n",
    "\n",
    "Here is the customer support ticket:    \n",
    "<ticket>{ticket}</ticket>\n",
    "\n",
    "Respond using this format:\n",
    "<category>The category label you chose goes here</category>\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1f2e886f-494d-4f51-8e00-72c2a18e2c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the training examples to the format expected by Fireworks.\n",
    "def training_examples_to_json(examples):\n",
    "    json_objs = list()\n",
    "    for idx, example in examples.iterrows():  \n",
    "        user_msg = create_prompt(example['text'])\n",
    "        asst_msg = f\"<category>{example['label']}</category>\"\n",
    "        msg = {\"messages\": [\n",
    "            {\"role\": \"user\", \"content\": user_msg}, \n",
    "            {\"role\": \"assistant\", \"content\": asst_msg}\n",
    "        ]}\n",
    "        json_objs.append(msg)\n",
    "    \n",
    "    return json_objs\n",
    "\n",
    "training_json = training_examples_to_json(training_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "68de340d-f246-4243-a205-5a4ebf2b7acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writes the data to a file so that it can be uploaded to Fireworks\n",
    "dataset_file_name = 'ticket-classification_training_data.jsonl'\n",
    "dataset_id = 'ticket-classification-v1'\n",
    "\n",
    "with open(dataset_file_name, 'w') as f:\n",
    "    for obj in training_json:\n",
    "        json.dump(obj, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c6a68299-9751-4263-ae87-1a9a531caf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow instructions here to first install the firectil CLI - https://readme.fireworks.ai/docs/fine-tuning-models#installing-firectl\n",
    "# Then run this command to upload the file to Fireworks\n",
    "!firectl create dataset {dataset_id} {dataset_file_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265fb601-ff13-40fb-967a-b8ebc098fd48",
   "metadata": {},
   "source": [
    "### Fine-Tuning\n",
    "\n",
    "We will now fine-tune models using the Fireworks API. Fireworks implements the QLoRA algorithm through a simple interface. Training parameters can be set via the --settings-file argument. In this exercise, we will fine-tune two models:\n",
    "- The first model will use Fireworks default training parameters\n",
    "- The second model will increase the rank, learning rate, and epochs to make fine-tuning more aggressive\n",
    "\n",
    "See https://docs.fireworks.ai/fine-tuning/fine-tuning-models#starting-your-tuning-job for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ee8dd75b-d1d4-4e9c-be55-e7c7777cbb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a training job with the default hyperparameters\n",
    "!firectl create fine-tuning-job --settings-file ticket_classification_v1.yaml --display-name ticket-classification-v1 --dataset {dataset_id} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5d541454-c4b0-4a4c-b058-4cb19cb48542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a training job with the increased rank, learning rate, and epochs\n",
    "!firectl create fine-tuning-job --settings-file ticket_classification_v2.yaml --display-name ticket-classification-v2 --dataset {dataset_id} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8565ac9-eb20-4070-8c7c-7796d199f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1 is the id of the training job with default hyperparameters, v2 is with the increased settings\n",
    "# NOTE THAT THESE IDS WILL CHANGE WHEN YOU RUN THE FINE-TUNING JOB ON YOUR ACCOUNT!!!\n",
    "# The model id is printed in the stdout of the cell above as Name: accounts/{account_id}/fineTuningJobs/{model_id}\n",
    "model_v1_id = 'e39cc15d4b80486ea089786b565cf7d1'\n",
    "model_v2_id = 'a2b729a82fc64095a3c487b392f5187b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "498a82de-3e66-47ec-bab4-b951ef86c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the State of the two fine-tuning jobs are listed as COMPLETED (~10-20 minutes)\n",
    "!firectl get fine-tuning-job {model_v1_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58df2a33-7d60-4ee7-8b2c-7c1d08c90ce9",
   "metadata": {},
   "source": [
    "### Evluate Results\n",
    "\n",
    "We will now deploy our models and evaluate the results. We will calculate the accuracy on three different models\n",
    "\n",
    "- The base model without any fine-tuning\n",
    "- Our first fine-tuned model, with the default hyperparameters\n",
    "- Our second fine-tuned model, with the more aggressive hyperparameters\n",
    "\n",
    "See https://docs.fireworks.ai/fine-tuning/fine-tuning-models#deploying-the-model-for-inference for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c6d45aab-a44f-4442-8e84-e14751b688d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the first model to a Fireworks serverless endpoint\n",
    "!firectl deploy {model_v1_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e9580299-573a-4727-8f7b-3d0f068cebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the second model to a Fireworks serverless endpoint\n",
    "!firectl deploy {model_v2_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f081c2cc-d5a5-4763-8902-5833522c7fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the the Deploymed Model Refs lists the state of the models as \"DEPLOYED\" (~5-20 minutes).\n",
    "!firectl get model {model_v1_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbba9937-f451-4677-98aa-9f489f0927d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses an LLM to predicted class labels for a list of support tickets\n",
    "def classify_tickets(tickets, model):\n",
    "    responses = list()\n",
    "\n",
    "    for ticket in tickets:\n",
    "        user_prompt = create_prompt(ticket)\n",
    "    \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                { \"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            # setting temperature to 0 for this use case, so that responses are as deterministic as possible\n",
    "            temperature=0, \n",
    "            stop=[\"</category>\"],\n",
    "            max_tokens=2048,\n",
    "        )\n",
    "        response = response.choices[0].message.content.split(\"<category>\")[-1].strip()\n",
    "        responses.append(response)\n",
    "\n",
    "    return responses\n",
    "\n",
    "\n",
    "# Calculates the percent of predictions we classified correctly\n",
    "def evaluate_accuracy(predicted, actual):\n",
    "    num_correct = sum([predicted[i] == actual[i] for i in range(len(actual))])\n",
    "    return round(100 * num_correct / len(actual), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e18e6cb6-cf83-4393-a804-a13360794b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 60.29%\n",
      "Test Set Accuracy: 60.29%\n"
     ]
    }
   ],
   "source": [
    "# Determine how the base model without any fine-tuning performs\n",
    "model_id = 'accounts/fireworks/models/llama-v3-8b-instruct'\n",
    "\n",
    "training_responses = classify_tickets(\n",
    "    tickets=training_tickets, \n",
    "    model=model_id\n",
    ")\n",
    "accuracy = evaluate_accuracy(training_responses, training_labels)\n",
    "print(f\"Training Set Accuracy: {accuracy}%\")\n",
    "\n",
    "test_responses = classify_tickets(\n",
    "    tickets=test_tickets, \n",
    "    model=model_id\n",
    ")\n",
    "\n",
    "accuracy = evaluate_accuracy(test_responses, test_labels)\n",
    "print(f\"Test Set Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "256d7c65-4050-43e0-881d-58a1ad398de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 69.12%\n",
      "Test Set Accuracy: 67.65%\n"
     ]
    }
   ],
   "source": [
    "# Determine how the fine-tuned model performs with the default fine-tuning params\n",
    "model_id = f'accounts/{account_id}/models/{model_v1_id}'\n",
    "\n",
    "training_responses = classify_tickets(\n",
    "    tickets=training_tickets, \n",
    "    model=model_id\n",
    ")\n",
    "accuracy = evaluate_accuracy(training_responses, training_labels)\n",
    "print(f\"Training Set Accuracy: {accuracy}%\")\n",
    "\n",
    "test_responses = classify_tickets(\n",
    "    tickets=test_tickets, \n",
    "    model=model_id\n",
    ")\n",
    "\n",
    "accuracy = evaluate_accuracy(test_responses, test_labels)\n",
    "print(f\"Test Set Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "198b6a3a-422e-4157-b5b2-b00f9ddcb106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 61.76%\n",
      "Test Set Accuracy: 55.88%\n"
     ]
    }
   ],
   "source": [
    "# Determine how the base model performs with the increases rank, epochs, and learning rate\n",
    "model_id = f'accounts/{account_id}/models/{model_v2_id}'\n",
    "\n",
    "training_responses = classify_tickets(\n",
    "    tickets=training_tickets, \n",
    "    model=model_id\n",
    ")\n",
    "accuracy = evaluate_accuracy(training_responses, training_labels)\n",
    "print(f\"Training Set Accuracy: {accuracy}%\")\n",
    "\n",
    "test_responses = classify_tickets(\n",
    "    tickets=test_tickets, \n",
    "    model=model_id\n",
    ")\n",
    "\n",
    "accuracy = evaluate_accuracy(test_responses, test_labels)\n",
    "print(f\"Test Set Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "86c759ab-f84e-44bb-b621-86550519c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undeploy the first model (does not cost anything extra, but Fireworks may limit your number of deployed models).\n",
    "!firectl undeploy {model_v1_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b09d5f87-0bcb-4078-a6ff-2a02c8ccc48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undeploy the second model (does not cost anything extra, but Fireworks may limit your number of deployed models).\n",
    "!firectl undeploy {model_v2_id}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
