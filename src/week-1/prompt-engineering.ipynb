{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b5b0b41-5df0-45bb-b825-e440c0cc3f76",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To complete the following guide you will need to install the following packages:\n",
    "- fireworks-ai\n",
    "- pandas\n",
    "\n",
    "You will also need:\n",
    "\n",
    "- Fireworks account\n",
    "- Fireworks API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f286740-3c83-4eb9-97e9-e62e2a1543dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fireworks-ai pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c70ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from fireworks.client import Fireworks\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28c65cb8-6dce-401b-b900-a7d3b298a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have the FIREWORKS_API_KEY environment variable set to your account's key!\n",
    "# os.environ['FIREWORKS_API_KEY'] = 'XXX'\n",
    "\n",
    "client = Fireworks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c48b9d3-2265-4705-a5fc-dca12dccc602",
   "metadata": {},
   "source": [
    "## Problem Definition: Insurance Support Ticket Classifier\n",
    "\n",
    "*Note: The problem definition, data, and labels used in this example were synthetically generated by Claude 3 Opus*\n",
    "\n",
    "In the insurance industry, customer support plays a crucial role in ensuring client satisfaction and retention. Insurance companies receive a high volume of support tickets daily, covering a wide range of topics such as billing, policy administration, claims assistance, and more. Manually categorizing these tickets can be time-consuming and inefficient, leading to longer response times and potentially impacting customer experience.\n",
    "\n",
    "### Task\n",
    "In this exercise, we will evaluate the accuracy of various prompts on the test.tsv dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b771ec57-1ec9-47e7-8ca5-7e90943f1c59",
   "metadata": {},
   "source": [
    "#### Labeled Data\n",
    "\n",
    "The data can be found in the week-1 `data` folder.\n",
    "\n",
    "We will use the following datasets:\n",
    "- `./data/train.tsv`\n",
    "- `./data/test.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a48d42d-b288-4a5d-929f-470f38213593",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples = pd.read_csv('data/train.tsv', sep='\\t')\n",
    "test_examples = pd.read_csv('data/test.tsv', sep='\\t')\n",
    "\n",
    "# In order to not leak information about the test labels into our prompts, the list of possible categories will be defined \n",
    "# based on the training labels. We'll discuss train/test splits more during week 2.\n",
    "categories = sorted(training_examples['label'].unique().tolist())\n",
    "categories_str = '\\n'.join(categories)\n",
    "\n",
    "test_tickets = test_examples['text'].tolist()\n",
    "test_labels = test_examples['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c596b097-aef6-4674-9b67-eef7e47bce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses an LLM to predicted class labels for a list of support tickets\n",
    "def classify_tickets(tickets, prompt_generator, model=\"accounts/fireworks/models/llama-v3p1-8b-instruct\"):\n",
    "    responses = list()\n",
    "\n",
    "    for ticket in tickets:\n",
    "        user_prompt = prompt_generator(ticket)\n",
    "    \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                { \"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            # setting temperature to 0 for this use case, so that responses are as deterministic as possible\n",
    "            temperature=0, \n",
    "            stop=[\"</category>\"],\n",
    "            max_tokens=2048,\n",
    "        )\n",
    "        response = response.choices[0].message.content.split(\"<category>\")[1].strip()\n",
    "        responses.append(response)\n",
    "\n",
    "    return responses\n",
    "\n",
    "\n",
    "# Calculates the percent of predictions we classified correctly\n",
    "def evaluate_accuracy(predicted, actual):\n",
    "    num_correct = sum([predicted[i] == actual[i] for i in range(len(actual))])\n",
    "    return round(100 * num_correct / len(actual), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac6dd0d-fe00-4263-b964-c05fe86105fe",
   "metadata": {},
   "source": [
    "### Classification with a simple prompt\n",
    "\n",
    "We will first evaluate the accuracy of the LLM on a simple prompt that does not used any advanced prompt engineering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a7c4842-9446-438e-ad1f-654a44ae0e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_prompt(ticket):\n",
    "    return f\"\"\"classify a customer support ticket into one of the following categories:\n",
    "<categories>\n",
    "{categories_str}\n",
    "</categories>\n",
    "\n",
    "Here is the customer support ticket:    \n",
    "<ticket>{ticket}</ticket>\n",
    "\n",
    "Respond using this format:\n",
    "<category>The category label you chose goes here</category>\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88b3a214-bd8d-45a5-a3cb-fc3390c86223",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_responses = classify_tickets(\n",
    "    tickets=test_tickets, \n",
    "    prompt_generator=create_simple_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3094aa9-80d6-4ef3-a708-4feab1679b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Prompt Accuracy: 60.29%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_accuracy(simple_responses, test_labels)\n",
    "print(f\"Initial Prompt Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc5aeec-daff-4e29-9c55-2b8437ec1563",
   "metadata": {},
   "source": [
    "### Classification with an improved prompt\n",
    "\n",
    "This prompt builds upon the simple prompt, and improves the accuracy of the classification by applying the following techniques:\n",
    "- chain-of-thought: makes the LLM reflect on its reasoning before providing a response\n",
    "- few-shot learning: provides examples within the context of the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8c4d5e9-ddb7-455d-9570-e911b270afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve one example from each of the k most popular labels\n",
    "def retrieve_few_shot_examples(df, k=5):\n",
    "    # Count the frequency of each label\n",
    "    label_counts = df['label'].value_counts()\n",
    "\n",
    "    # Identify the k most common labels\n",
    "    top_labels = label_counts.head(k).index\n",
    "\n",
    "    # Retrieve a single row for each of these labels\n",
    "    rows = df[df['label'].isin(top_labels)].groupby('label').first().reset_index()\n",
    "\n",
    "    # Convert each row to the example string format required by the prompt\n",
    "    example_strs = list()\n",
    "    for idx, row in rows.iterrows():\n",
    "        example_strs.append(f\"<example><ticket>{row['text']}</ticket><label>{row['label']}</label></example>\")\n",
    "\n",
    "    return '\\n'.join(example_strs)\n",
    "\n",
    "few_shot_examples = retrieve_few_shot_examples(training_examples)\n",
    "\n",
    "def create_improved_prompt(ticket):\n",
    "    return f\"\"\"classify a customer support ticket into one of the following categories:\n",
    "<categories>\n",
    "{categories_str}\n",
    "</categories>\n",
    "\n",
    "Here is the customer support ticket:    \n",
    "<ticket>{ticket}</ticket>\n",
    "\n",
    "Use the following examples to help you classify the query:\n",
    "<examples>\n",
    "{few_shot_examples}\n",
    "</examples>\n",
    "\n",
    "First you will think step-by-step about the problem in the scratchpad tag.\n",
    "You should consider all the information provided and create a concrete argument for your classification.\n",
    "\n",
    "Respond using this format:\n",
    "<response>\n",
    "  <scratchpad>Your thoughts and analysis go here</scratchpad>\n",
    "  <category>The category label you chose goes here</category>\n",
    "</response>\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb438a76-33fd-4b0f-9715-8dc526ad3a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_responses = classify_tickets(\n",
    "    tickets=test_tickets, \n",
    "    prompt_generator=create_improved_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65177c58-6d9b-4430-96dc-d8057e020fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Prompt Accuracy: 63.24%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_accuracy(improved_responses, test_labels)\n",
    "print(f\"Improved Prompt Accuracy: {accuracy}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
