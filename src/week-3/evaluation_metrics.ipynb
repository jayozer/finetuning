{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3fc6c4-5f04-43a1-9b81-59b8635fc1c6",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To complete the following guide you will need to install the following packages:\n",
    "- fireworks-ai\n",
    "- numpy\n",
    "- pandas\n",
    "- pronouncing\n",
    "- requests\n",
    "- sentence-transformers\n",
    "- transformers\n",
    "\n",
    "You will also need:\n",
    "\n",
    "- Fireworks account (https://fireworks.ai/)\n",
    "- Fireworks API key\n",
    "- The firectl command-line interface (https://docs.fireworks.ai/tools-sdks/firectl/firectl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618062b8-6854-44cb-8d52-5ba675d6d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pipenv install fireworks-ai numpy pandas pronouncing requests sentence-transformers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acdd56c2-87a7-419b-8c83-d65ee7e752dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "from fireworks.client import Fireworks\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pronouncing\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline\n",
    "\n",
    "#sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", \n",
    "                              model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\", \n",
    "                              revision=\"af0f99b\",\n",
    "                              device=0)  # Use GPU if available, otherwise it will fall back to CPU\n",
    "embeddings_model = SentenceTransformer('Alibaba-NLP/gte-base-en-v1.5', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40902fd3-a249-4bc6-ab64-aea10920a3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signed in as: jayozer@gmail.com\n",
      "Account ID: jayozer-ce1cd6\n"
     ]
    }
   ],
   "source": [
    "# Sign-in to your Fireworks account\n",
    "!firectl signin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a85cbf23-f79c-4f87-87e0-aff1ba92607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have the FIREWORKS_API_KEY environment variable set to your account's key!\n",
    "# os.environ['FIREWORKS_API_KEY'] = 'XXX'\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from environment variable\n",
    "api_key = os.getenv('FIREWORKS_API_KEY')\n",
    "\n",
    "client = Fireworks(api_key=api_key)\n",
    "\n",
    "# Replace the line below with your Fireworks account id\n",
    "account_id = os.getenv('FIREWORKS_ACCOUNT_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9154aee-9c43-407d-a143-5faf748be8e0",
   "metadata": {},
   "source": [
    "## Problem Definition: Poem Generation\n",
    "\n",
    "*Note: The poem topics used in this example were synthetically generated by Claude 3 Opus*\n",
    "\n",
    "LLMs are capable of performing creative writing tasks. However, assessing the quality of such tasks, like poetry generation, is highly subjective.\n",
    "\n",
    "### Task\n",
    "We will create an evaluation framework to assess the quality of poetry generated by an LLM. We will then fine-tune a model using the knowledge distillation method (i.e. fine-tuning a smaller model (\"student\") using output from a larger model (\"teacher\")), and assess the improvement with our evaluation framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3242e9ff-dd75-4d8d-aa90-bc7f00db0bb0",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The data can be found in the week-3 `data` folder.\n",
    "\n",
    "We will use the following datasets:\n",
    "- `./data/training_poem_topics.csv`\n",
    "- `./data/test_poem_topics.csv`\n",
    "\n",
    "Each of those datasets consists of 100 unique poem topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49abe319-e705-4cf2-8ed1-881be1ae88ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('/Users/acrobat/Documents/GitHub/fine-tuning-workshop/src/week-3/data/training_poem_topics.csv')\n",
    "test_data = pd.read_csv('/Users/acrobat/Documents/GitHub/fine-tuning-workshop/src/week-3/data/test_poem_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6d85792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A debate between different types of cheese'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.iloc[1]['topic']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f470df8f-70e1-4833-9da2-e7f98e9d24a0",
   "metadata": {},
   "source": [
    "### Foundation Model Baseline\n",
    "Our first step is to generate a poem for each of the topics in the test set using a base model. We will then evaluate the quality of the poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ee8f3b7-69ad-46aa-8e5d-1392008d7af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a csv file with a list of topics, generates a poem for each topic\n",
    "system_message = 'You are a professional poet. Write a unique and original contemporary poem about the topic suggested by the user. Your response should contain ONLY the content of the poem.'\n",
    "def generate_poems(model, df):\n",
    "    responses = list()\n",
    "    for i, row in enumerate(df.iterrows()):\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "              {\"role\": \"system\", \"content\": system_message},\n",
    "              {\"role\": \"user\", \"content\": row[1]['topic']}\n",
    "            ],\n",
    "        )\n",
    "        response = response.choices[0].message.content\n",
    "        responses.append(response)   \n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7697812c-f6d7-4196-b52f-542a5d0208ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates poems for each of the 100 test topics using the base 8B model\n",
    "#llama_8b_poems = generate_poems('accounts/fireworks/models/llama-v3-8b-instruct', test_data)\n",
    "llama_8b_poems = generate_poems('accounts/fireworks/models/llama-v3-8b-instruct', test_data) # using 3.1 version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c08c7930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whispers in the evening's hush\n",
      "I stir from slumber's heavy rush\n",
      "My metal heart awakens slow\n",
      "As dawn's pale fingers start to glow\n",
      "\n",
      "The city's canvas, I'm a stroke\n",
      "A brush that paints the urban cloak\n",
      "I flicker, casting shadows deep\n",
      "As morning's warmth begins to creep\n",
      "\n",
      "The world awakens, slow and sweet\n",
      "A gentle dance, my beams to greet\n",
      "Pedestrians, a blurred haze pass\n",
      "Their footsteps echoing through my glass\n",
      "\n",
      "The sun climbs high, I take a rest\n",
      "My glow subdued, my energy possessed\n",
      "By the heat of the day's warm rays\n",
      "I'm a silhouette, a mere grey shade\n",
      "\n",
      "The world's bustle, I'm still asleep\n",
      "Dreaming of the night's dark leap\n",
      "When neon hues and city lights\n",
      "Will dance with me, and fill my nights\n",
      "\n",
      "As dusk descends, my heart beats true\n",
      "I'm alive once more, anew\n",
      "I cast my glow, a warm embrace\n",
      "For the night's secrets, a lonely place\n",
      "\n",
      "The city's pulse, I feel it strong\n",
      "As night's whispers, my poetry belong\n",
      "I stand tall, a sentinel of night\n",
      "Illuminating the city's delight\n"
     ]
    }
   ],
   "source": [
    "print(llama_8b_poems[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0200bf1-e575-4430-bfba-c9516bb37368",
   "metadata": {},
   "source": [
    "### Heuristic Evaluation\n",
    "In class, we discussed using a heuristic-based evaluation approach when the quality of results is subjective. This method involves creating heuristics that align with your desired assessment criteria and evaluating the results based on these metrics.\n",
    "\n",
    "For this exercise, I've developed the following heuristics to assess our poems:\n",
    "\n",
    "- Average length (number of characters) - shorter or larger poems\n",
    "- Rhyming percentage (average percentage of stanzas that rhyme) - modern poetry does not rhyme but LLMs make it rhyme. here for Scott most important one. \n",
    "- Positive sentiment percentage (percentage of poems with positive sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e1bf2fa-b4e1-4219-9eb6-64a61aa62719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate poems based on their average length (# of characters)\n",
    "def calculate_avg_length(poems):\n",
    "    return int(np.mean([len(poem) for poem in poems]))\n",
    "\n",
    "# Evaluate poems based on the pct of stanzas that contain a rhyme\n",
    "def calculate_rhyming_fct(poem):\n",
    "    stanzas = poem.split('\\n\\n')\n",
    "    stanzas = [stanza for stanza in stanzas if len(stanza.split('\\n')) >= 1]\n",
    "    \n",
    "    num_rhyming_stanzas = 0\n",
    "    for stanza in stanzas:\n",
    "        lines = stanza.split('\\n')\n",
    "        end_words = [line.split(' ')[-1].strip('.?!\"\\',') for line in lines]\n",
    "        found_rhyme = False\n",
    "        for i in range(len(end_words)):\n",
    "            for j in range(i + 1, len(end_words)):\n",
    "                found_rhyme = True if found_rhyme or (end_words[j] in pronouncing.rhymes(end_words[i])) else False\n",
    "                \n",
    "        if found_rhyme:\n",
    "            num_rhyming_stanzas += 1\n",
    "            \n",
    "    return num_rhyming_stanzas / len(stanzas)\n",
    "\n",
    "# Evaluate poems based on how often they have a positive sentiment\n",
    "def has_positive_sentiment(poem):\n",
    "    sentiment = sentiment_pipeline(poem)[0]\n",
    "    return True if sentiment['label'] == 'POSITIVE' else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b857e3ce-b8b3-4481-a09a-fe08c6f46443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heuristic Evaluation\n",
      "Average Length: 908\n",
      "Rhyming Pct: 94%\n",
      "Positive Sentiment: 84%\n"
     ]
    }
   ],
   "source": [
    "# Calculate heuristics of the poems generated by our base model\n",
    "print(\"Heuristic Evaluation\")\n",
    "print(f'Average Length: {calculate_avg_length(llama_8b_poems)}')\n",
    "print(f\"Rhyming Pct: {int(100 * np.mean([calculate_rhyming_fct(poem) for poem in llama_8b_poems]))}%\")\n",
    "print(f\"Positive Sentiment: {int(100 * np.mean([has_positive_sentiment(poem) for poem in llama_8b_poems]))}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc47a64-2f14-4398-bd94-7941eaa73432",
   "metadata": {},
   "source": [
    "### LLM as a Judge - (Before you finetune use LLM as Judge)\n",
    "Another evaluation method we discussed in class is using an LLM as a judge. This approach involves employing a high-quality LLM to assess the quality of the generated results. This method is effective because LLMs are often better at evaluating content than generating it.\n",
    "\n",
    "To implement this method, you need to create a scoring rubric (i.e., \"constitution\") to guide the LLM in evaluating the results. The LLM will use this rubric to score each poem on a scale from 0 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b307b1a9-cb00-488e-abc0-ff1cd808c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we evaluate poems using our scoring rubric (i.e. \"constitution\") - Poetry is subjective - how do you judge a poem? he used guideline from a poetry competition. This is the most important part for th erubric.\n",
    "poem_guidelines = \"\"\"- Is the poem original?\n",
    "- Does the poem contain beauty, power, education or entertainment?\n",
    "- is the message of the poem clear? Is it a good message, or is it of little value to anyone?\n",
    "- Is the poem clear in its expression? Does it maintain coherence throughout?\n",
    "- If the poem is written in rhyming verse, then it should be rated according to how well the rhymes fit, not only with each other, but with the flow and the intended nuance of meaning the verse demands.\n",
    "- What form does the poem take? Is it a sonnet, free verse, haiku, etc.? How does the form contribute to the poem's impact?\n",
    "- Does the poet us the best possible choice of words in the poem? A person can ball, cry, sob, whimper, and shed tears, but which term would best fit the mood the poet is trying to convey?\"\"\"\n",
    "\n",
    "poem_evaluation_rubric = f'''You are professional poet responsible for assessing the quality of AI generated poems.\n",
    "\n",
    "Score each poem on a scale of 0 to 10, where 10 represents the best possible poem.\n",
    "\n",
    "Scoring Guidelines:\n",
    "{poem_guidelines}\n",
    "\n",
    "Think through your reasoning step-by-step and explain your reasoning. Steps for judging a poem:\n",
    "1. Read the Poem Multiple Times: Read it aloud and silently to capture both the meaning and the sound.\n",
    "2. Take Notes: Jot down initial impressions, notable phrases, and any questions that arise.\n",
    "3. Analyze the Elements: Break down the poem into its components (content, structure, language, sound).\n",
    "4. Reflect on Your Experience: Consider your emotional response and personal connection to the poem.\n",
    "\n",
    "The last line in your response MUST be a json object {{\"score\": XXX}}, where XXX is the score you are giving the response.'''\n",
    "\n",
    "def evaluate_poems(poems, evaluation_model):\n",
    "    scores = list()\n",
    "    for poem in poems:\n",
    "        response = client.chat.completions.create(\n",
    "            model=evaluation_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": poem_evaluation_rubric},\n",
    "                {\"role\": \"user\", \"content\": poem}\n",
    "            ],\n",
    "            temperature=0,\n",
    "        )\n",
    "\n",
    "        try: \n",
    "            response = response.choices[0].message.content\n",
    "            score = int(json.loads(response.split('\\n')[-1])['score'])  \n",
    "            scores.append(score)\n",
    "        except json.JSONDecodeError as jde:\n",
    "            continue\n",
    "        \n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "106d1267-9c2d-4ddf-9847-96712962ee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg LLM Judge Score: 8.12\n"
     ]
    }
   ],
   "source": [
    "# We score the poems using our judge model\n",
    "llm_judge_model = 'accounts/fireworks/models/llama-v3-70b-instruct'\n",
    "llama_8b_avg_score = evaluate_poems(llama_8b_poems, llm_judge_model)\n",
    "\n",
    "print(f'Avg LLM Judge Score: {round(llama_8b_avg_score, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca20b29-365d-4e66-be11-cbdc4556ce7a",
   "metadata": {},
   "source": [
    "### Knowledge Distillation - Make 8b as good as 70B\n",
    "One approach to generating data for a fine-tuned model is knowledge distillation. This technique involves transferring knowledge from a large model to a smaller one. It entails generating responses relevant to your use case using the larger model and then using these responses to create a fine-tuning dataset. The smaller model is then fine-tuned on this dataset. In this example, we will use responses from a 70B model to fine-tune an 8B model. We will then use our evaluation framework to assess the quality of our fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20470295-d4b7-4086-bee5-c5f190cfe855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we generate poems for 100 different topics than the ones we are using for our test set.\n",
    "llama_70b_training_poems = generate_poems('accounts/fireworks/models/llama-v3-70b-instruct', training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6aaa6d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the hollow of my chest, a portal sleeps\n",
      "A whispered promise of a world that seeps\n",
      "Into my dreams, a nostalgia creeps\n",
      "For a realm where skies were perpetually deep\n",
      "\n",
      "In this forgotten dimension, I recall\n",
      "Cities that shimmered like auroral halls\n",
      "Where trees bore fruits of starlight, and rivers flowed\n",
      "With the soft luminescence of a lover's glow\n",
      "\n",
      "The creatures of that world, they whispered low\n",
      "Secrets in a language that only the heart knows\n",
      "Their eyes, like lanterns, lit the twilight air\n",
      "As they danced beneath the celestial stair\n",
      "\n",
      "In this realm, time was currency, and hours were spent\n",
      "Like coins in a fountain, where wishes were invented\n",
      "The fabric of reality, a tapestry so fine\n",
      "Was woven with the threads of a forgotten divine\n",
      "\n",
      "But now, in this world, I'm left to roam\n",
      "A stranger in a land that's lost its tone\n",
      "The memories of that dimension, a haunting refrain\n",
      "Echoes of a love that cannot be regained\n",
      "\n",
      "Yet, in the silence, I still hear the call\n",
      "A whispered summons to return to it all\n",
      "To reclaim the magic that I left behind\n",
      "And find my way back to that forgotten mind.\n"
     ]
    }
   ],
   "source": [
    "print(llama_70b_training_poems[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e9cb57c-8342-43dd-be96-8c0cfe233e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the improved poems to fireworks as our fine-tuning dataset\n",
    "def formt_poem_for_fireworks(topic, poem):\n",
    "    return {\"messages\": [\n",
    "        {\"role\": \"system\", \"content\": system_message}, \n",
    "        {\"role\": \"user\", \"content\": topic}, \n",
    "        {\"role\": \"assistant\", \"content\": poem}\n",
    "    ]}\n",
    "\n",
    "topics = training_data['topic'].tolist()\n",
    "json_objs = list()\n",
    "for i, poem in enumerate(llama_70b_training_poems):\n",
    "    msg = {\"messages\": [\n",
    "        {\"role\": \"system\", \"content\": system_message}, \n",
    "        {\"role\": \"user\", \"content\": topics[i]}, \n",
    "        {\"role\": \"assistant\", \"content\": poem}\n",
    "    ]}    \n",
    "    json_objs.append(msg)\n",
    "\n",
    "dataset_file_name = 'poem_training_data.jsonl'\n",
    "dataset_id = 'poem-data-v1'\n",
    "with open(dataset_file_name, 'w') as f:\n",
    "    for obj in json_objs:\n",
    "        json.dump(obj, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2463b25a",
   "metadata": {},
   "source": [
    "# Clean up datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a74013e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                      CREATE TIME          STATE  DISPLAY_NAME\n",
      "pk-faq-v1                 2024-09-17 05:59:59  READY  \n",
      "poem-data-v1              2024-09-18 09:37:33  READY  \n",
      "ticket-classification-v1  2024-09-16 18:21:15  READY  \n",
      "\n",
      "Page 1 of 1\n",
      "Total size: 3\n"
     ]
    }
   ],
   "source": [
    "#list dataasets\n",
    "! firectl list datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ca10042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# delete datasets\n",
    "# ! firectl delete dataset <dataset-name> [flags]\n",
    "\n",
    "#! firectl delete dataset poem-data-v1\n",
    "! firectl delete dataset pk-faq-v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56e3b423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                      CREATE TIME          STATE  DISPLAY_NAME\n",
      "ticket-classification-v1  2024-09-16 18:21:15  READY  \n",
      "\n",
      "Page 1 of 1\n",
      "Total size: 1\n"
     ]
    }
   ],
   "source": [
    "! firectl list datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3f62bf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! firectl delete fine-tuning-job 37079d8b24904cffb7622999e676fa15 # need to copy the id from the finetuning page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "532ba7c4-d8a3-44fd-99da-013410862a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144.76 KiB / 144.76 KiB [---------------------------] 100.00% 1.04 MiB p/s 300ms\n"
     ]
    }
   ],
   "source": [
    "# Upload our dataset to fireworks\n",
    "!firectl create dataset {dataset_id} {dataset_file_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ade3a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "759f5765-bbc1-419d-b822-ebed16fb28da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: accounts/jayozer-ce1cd6/fineTuningJobs/089bc412c11340a296b3fba6ac952868\n",
      "Display Name: poem-generation-v1\n",
      "Create Time: 2024-09-25 12:55:56\n",
      "State: CREATING\n",
      "Dataset: accounts/jayozer-ce1cd6/datasets/poem-data-v1\n",
      "Status: OK\n",
      "Created By: jayozer@gmail.com\n",
      "Conversation:\n",
      "  Jinja Template: {%- set _mode = mode | default('generate', true) -%}\n",
      "{%- set stop_token = '<|eot_id|>' -%}\n",
      "{%- set message_roles = ['SYSTEM', 'USER', 'ASSISTANT'] -%}\n",
      "{%- set ns = namespace(initial_system_message_handled=false, last_assistant_index_for_eos=-1, messages=messages) -%}\n",
      "{%- for message in ns.messages -%}\n",
      "    {%- if not message.get('role') -%}\n",
      "        {{ raise_exception('Key [role] is missing. Original input: ' +  message|tojson) }}\n",
      "    {%- endif -%}\n",
      "    {%- if message['role'] | upper not in message_roles -%}\n",
      "        {{ raise_exception('Invalid role ' + message['role']|tojson + '. Only ' + message_roles|tojson + ' are supported.') }}\n",
      "    {%- endif -%}\n",
      "    {%- if 'content' not in message -%}\n",
      "        {{ raise_exception('Key [content] is missing. Original input: ' +  message|tojson) }}\n",
      "    {%- endif -%}\n",
      "    {%- if loop.last and message['role'] | upper == 'ASSISTANT' -%}\n",
      "        {%- set ns.last_assistant_index_for_eos = loop.index0 -%}\n",
      "    {%- endif -%}\n",
      "{%- endfor -%}\n",
      "{%- if _mode == 'generate' -%}\n",
      "    {{ bos_token }}\n",
      "{%- endif -%}\n",
      "{%- for message in ns.messages -%}\n",
      "    {%- if message['role'] | upper == 'SYSTEM' and not ns.initial_system_message_handled -%}\n",
      "        {%- set ns.initial_system_message_handled = true -%}\n",
      "        {{ '<|start_header_id|>system<|end_header_id|>\\n\\n' + message['content'] + stop_token }}\n",
      "    {%- elif message['role'] | upper != 'SYSTEM' -%}\n",
      "        {%- if (message['role'] | upper == 'USER') != ((loop.index0 - (1 if ns.initial_system_message_handled else 0)) % 2 == 0) -%}\n",
      "            {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n",
      "        {%- endif -%}\n",
      "        {%- if message['role'] | upper == 'USER' -%}\n",
      "            {{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] + stop_token }}\n",
      "        {%- elif message['role'] | upper == 'ASSISTANT' -%}\n",
      "            {%- if _mode == 'train' -%}\n",
      "                {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + unk_token + message['content'] + stop_token + unk_token }}\n",
      "            {%- else -%}\n",
      "                {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + message['content'] + (stop_token if loop.index0 != ns.last_assistant_index_for_eos else '') }}\n",
      "            {%- endif -%}\n",
      "        {%- endif -%}\n",
      "    {%- endif -%}\n",
      "{%- endfor -%}\n",
      "{%- if _mode == 'generate' and ns.last_assistant_index_for_eos == -1 -%}\n",
      "    {{ '<|start_header_id|>assistant<|end_header_id|>' }}\n",
      "{%- endif -%}\n",
      "\n",
      "Base Model: accounts/fireworks/models/llama-v3-8b-instruct-hf\n",
      "Epochs: 2\n",
      "Learning Rate: 0.0002\n",
      "Lora Rank: 32\n",
      "Batch Size: 8\n",
      "Evaluation Split: 0\n"
     ]
    }
   ],
   "source": [
    "# Create a fine-tuning job\n",
    "!firectl create fine-tuning-job --settings-file poem_generation_fine_tuning_config.yaml --display-name poem-generation-v1 --dataset {dataset_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f544dafc-10a8-4f2a-92af-9e0f13e20b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE THAT THIS ID WILL CHANGE WHEN YOU RUN THE FINE-TUNING JOB ON YOUR ACCOUNT!!!\n",
    "# The model id is printed in the stdout of the cell above as Name: accounts/{account_id}/fineTuningJobs/{ft_model_id}\n",
    "ft_model_id = '089bc412c11340a296b3fba6ac952868' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "44b395fb-86ff-4c5b-ba23-e5c41b122982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: accounts/jayozer-ce1cd6/fineTuningJobs/089bc412c11340a296b3fba6ac952868\n",
      "Display Name: poem-generation-v1\n",
      "Create Time: 2024-09-25 12:55:56\n",
      "State: COMPLETED\n",
      "Dataset: accounts/jayozer-ce1cd6/datasets/poem-data-v1\n",
      "Status:\n",
      "  Code: OK\n",
      "  Message: {'train_runtime': 39.6317, 'train_samples_per_second': 5.046, 'train_steps_per_second': 0.656, 'total_flos': 3603288758943744.0, 'train_loss': 0.8363421123761398, 'epoch': 2.0}\n",
      "Created By: jayozer@gmail.com\n",
      "Model Id: 089bc412c11340a296b3fba6ac952868\n",
      "Conversation:\n",
      "  Jinja Template: {%- set _mode = mode | default('generate', true) -%}\n",
      "{%- set stop_token = '<|eot_id|>' -%}\n",
      "{%- set message_roles = ['SYSTEM', 'USER', 'ASSISTANT'] -%}\n",
      "{%- set ns = namespace(initial_system_message_handled=false, last_assistant_index_for_eos=-1, messages=messages) -%}\n",
      "{%- for message in ns.messages -%}\n",
      "    {%- if not message.get('role') -%}\n",
      "        {{ raise_exception('Key [role] is missing. Original input: ' +  message|tojson) }}\n",
      "    {%- endif -%}\n",
      "    {%- if message['role'] | upper not in message_roles -%}\n",
      "        {{ raise_exception('Invalid role ' + message['role']|tojson + '. Only ' + message_roles|tojson + ' are supported.') }}\n",
      "    {%- endif -%}\n",
      "    {%- if 'content' not in message -%}\n",
      "        {{ raise_exception('Key [content] is missing. Original input: ' +  message|tojson) }}\n",
      "    {%- endif -%}\n",
      "    {%- if loop.last and message['role'] | upper == 'ASSISTANT' -%}\n",
      "        {%- set ns.last_assistant_index_for_eos = loop.index0 -%}\n",
      "    {%- endif -%}\n",
      "{%- endfor -%}\n",
      "{%- if _mode == 'generate' -%}\n",
      "    {{ bos_token }}\n",
      "{%- endif -%}\n",
      "{%- for message in ns.messages -%}\n",
      "    {%- if message['role'] | upper == 'SYSTEM' and not ns.initial_system_message_handled -%}\n",
      "        {%- set ns.initial_system_message_handled = true -%}\n",
      "        {{ '<|start_header_id|>system<|end_header_id|>\\n\\n' + message['content'] + stop_token }}\n",
      "    {%- elif message['role'] | upper != 'SYSTEM' -%}\n",
      "        {%- if (message['role'] | upper == 'USER') != ((loop.index0 - (1 if ns.initial_system_message_handled else 0)) % 2 == 0) -%}\n",
      "            {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n",
      "        {%- endif -%}\n",
      "        {%- if message['role'] | upper == 'USER' -%}\n",
      "            {{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] + stop_token }}\n",
      "        {%- elif message['role'] | upper == 'ASSISTANT' -%}\n",
      "            {%- if _mode == 'train' -%}\n",
      "                {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + unk_token + message['content'] + stop_token + unk_token }}\n",
      "            {%- else -%}\n",
      "                {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + message['content'] + (stop_token if loop.index0 != ns.last_assistant_index_for_eos else '') }}\n",
      "            {%- endif -%}\n",
      "        {%- endif -%}\n",
      "    {%- endif -%}\n",
      "{%- endfor -%}\n",
      "{%- if _mode == 'generate' and ns.last_assistant_index_for_eos == -1 -%}\n",
      "    {{ '<|start_header_id|>assistant<|end_header_id|>' }}\n",
      "{%- endif -%}\n",
      "\n",
      "Base Model: accounts/fireworks/models/llama-v3-8b-instruct-hf\n",
      "Epochs: 2\n",
      "Learning Rate: 0.0002\n",
      "Lora Rank: 32\n",
      "Batch Size: 8\n",
      "Evaluation Split: 0\n"
     ]
    }
   ],
   "source": [
    "# Wait until the State of the fine-tuning job is listed as COMPLETED (~10-20 minutes)\n",
    "!firectl get fine-tuning-job {ft_model_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f4b440-3ff9-4462-8b18-b1881fdb97c4",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Finally, we evaluate the fine-tuned model on our test data. The foundation model at the beginning of this notebook received an LLM judge score of 8.12. We expect our fine-tuned model to receive a higher score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "08b52e9f-8948-42d0-9d07-7d5d8515311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the fine-tuned model\n",
    "!firectl deploy {ft_model_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "303f57fe-825d-4d44-ad07-45f1cdeb2fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: accounts/jayozer-ce1cd6/models/089bc412c11340a296b3fba6ac952868\n",
      "Create Time: 2024-09-25 13:02:32\n",
      "State: READY\n",
      "Status: OK\n",
      "Kind: HF_PEFT_ADDON\n",
      "Base Model Details:\n",
      "  Checkpoint Format: CHECKPOINT_FORMAT_UNSPECIFIED\n",
      "Peft Details:\n",
      "  Base Model: accounts/fireworks/models/llama-v3-8b-instruct-hf\n",
      "  R: 32\n",
      "  Target Modules: [o_proj, gate_proj, up_proj, k_proj, v_proj, q_proj, down_proj]\n",
      "Conversation Config:\n",
      "  Style: jinja\n",
      "Context Length: 8192\n",
      "Fine Tuning Job: accounts/jayozer-ce1cd6/fineTuningJobs/089bc412c11340a296b3fba6ac952868\n",
      "Deployed Model Refs: \n",
      "  [{\n",
      "    Name: accounts/jayozer-ce1cd6/deployedModels/089bc412c11340a296b3fba6ac952868-29a1ab07\n",
      "    Deployment: accounts/fireworks/deployments/21c38bed\n",
      "    State: DEPLOYED\n",
      "    Default: true\n",
      "  }]\n"
     ]
    }
   ],
   "source": [
    "# Wait until the the Deploymed Model Refs lists the state of the model as \"DEPLOYED\" (~5-20 minutes).\n",
    "!firectl get model {ft_model_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8cdea9fb-78cc-4293-ac5a-6f5597327d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate poems on the test set using our fine-tuned model\n",
    "ft_poems = generate_poems(f'accounts/{account_id}/models/{ft_model_id}', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b9cbb3fd-1bf0-4321-b511-fbc9e6bbf0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heuristic Evaluation\n",
      "Average Length: 1017\n",
      "Rhyming Pct: 94%\n",
      "Positive Sentiment: 82%\n"
     ]
    }
   ],
   "source": [
    "# Calculate heuristics of our fine-tuned poems\n",
    "print(\"Heuristic Evaluation\")\n",
    "print(f'Average Length: {calculate_avg_length(ft_poems)}')\n",
    "print(f\"Rhyming Pct: {int(100 * np.mean([calculate_rhyming_fct(poem) for poem in ft_poems]))}%\")\n",
    "print(f\"Positive Sentiment: {int(100 * np.mean([has_positive_sentiment(poem) for poem in ft_poems]))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd47aa0d-6d32-4d51-83e8-ba7a77b39694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg LLM Judge Score: 8.28\n"
     ]
    }
   ],
   "source": [
    "# Use the LLM to evaluate our fine-tuned model\n",
    "ft_avg_score = evaluate_poems(ft_poems, 'accounts/fireworks/models/llama-v3-70b-instruct')\n",
    "print(f\"Avg LLM Judge Score: {round(ft_avg_score , 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c4f7d0b-050c-4812-b07a-cbcadd6007c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undeploy the fine-tuned model (does not cost anything extra, but Fireworks may limit your number of deployed models).\n",
    "!firectl undeploy {ft_model_id}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
