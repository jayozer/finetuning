{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3fc6c4-5f04-43a1-9b81-59b8635fc1c6",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To complete the following guide you will need to install the following packages:\n",
    "- fireworks-ai\n",
    "- numpy\n",
    "- pandas\n",
    "- pronouncing\n",
    "- requests\n",
    "- sentence-transformers\n",
    "- transformers\n",
    "\n",
    "You will also need:\n",
    "\n",
    "- Fireworks account (https://fireworks.ai/)\n",
    "- Fireworks API key\n",
    "- The firectl command-line interface (https://docs.fireworks.ai/tools-sdks/firectl/firectl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618062b8-6854-44cb-8d52-5ba675d6d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipenv install fireworks-ai numpy pandas pronouncing requests sentence-transformers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acdd56c2-87a7-419b-8c83-d65ee7e752dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/acrobat/.local/share/virtualenvs/fine-tuning-workshop-WXOdYiJS/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b315d88b954c0ab7e977b10d022690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504e0bed4ea44588839963529b4ee14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6afa2d35064149b4d4faf894d3689e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd4eb4bb9bd47f4b737cf07c1ee0737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/acrobat/.local/share/virtualenvs/fine-tuning-workshop-WXOdYiJS/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8135ae94663e49eb92ebf2c24e75c4b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40be6eeefe541058f27bd279a9235b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/71.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6938d804782144e1a0dd168ddae2d0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83009e4a4d94472bb7a64dfa4d6e507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef83d8f72f94f799c1c33bb75ddc0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration.py:   0%|          | 0.00/7.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- configuration.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a9056045a44b3eb2312c927e3e5201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling.py:   0%|          | 0.00/59.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- modeling.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16929956ad8f460e919784b1975bb685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/547M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d810d774a24f629419ddfac50b9779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de30a164e0fa40168b086293a71c5343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703d634ebe8e4ea9a187f556519c379a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a734795a654b82bf7b4c0c682527e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09eb5b3bd0a48a8a917249376c1219e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "from fireworks.client import Fireworks\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pronouncing\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "embeddings_model = SentenceTransformer('Alibaba-NLP/gte-base-en-v1.5', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40902fd3-a249-4bc6-ab64-aea10920a3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signed in as: jayozer@gmail.com\n",
      "Account ID: jayozer-ce1cd6\n"
     ]
    }
   ],
   "source": [
    "# Sign-in to your Fireworks account\n",
    "!firectl signin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a85cbf23-f79c-4f87-87e0-aff1ba92607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have the FIREWORKS_API_KEY environment variable set to your account's key!\n",
    "# os.environ['FIREWORKS_API_KEY'] = 'XXX'\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from environment variable\n",
    "api_key = os.getenv('FIREWORKS_API_KEY')\n",
    "\n",
    "client = Fireworks(api_key=api_key)\n",
    "\n",
    "# Replace the line below with your Fireworks account id\n",
    "account_id = os.getenv('FIREWORKS_ACCOUNT_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9154aee-9c43-407d-a143-5faf748be8e0",
   "metadata": {},
   "source": [
    "## Problem Definition: Poem Generation\n",
    "\n",
    "*Note: The poem topics used in this example were synthetically generated by Claude 3 Opus*\n",
    "\n",
    "LLMs are capable of performing creative writing tasks. However, assessing the quality of such tasks, like poetry generation, is highly subjective.\n",
    "\n",
    "### Task\n",
    "We will create an evaluation framework to assess the quality of poetry generated by an LLM. We will then fine-tune a model using the knowledge distillation method (i.e. fine-tuning a smaller model (\"student\") using output from a larger model (\"teacher\")), and assess the improvement with our evaluation framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3242e9ff-dd75-4d8d-aa90-bc7f00db0bb0",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The data can be found in the week-3 `data` folder.\n",
    "\n",
    "We will use the following datasets:\n",
    "- `./data/training_poem_topics.csv`\n",
    "- `./data/test_poem_topics.csv`\n",
    "\n",
    "Each of those datasets consists of 100 unique poem topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49abe319-e705-4cf2-8ed1-881be1ae88ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('data/training_poem_topics.csv')\n",
    "test_data = pd.read_csv('data/test_poem_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6d85792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A debate between different types of cheese'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.iloc[1]['topic']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f470df8f-70e1-4833-9da2-e7f98e9d24a0",
   "metadata": {},
   "source": [
    "### Foundation Model Baseline\n",
    "Our first step is to generate a poem for each of the topics in the test set using a base model. We will then evaluate the quality of the poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ee8f3b7-69ad-46aa-8e5d-1392008d7af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a csv file with a list of topics, generates a poem for each topic\n",
    "system_message = 'You are a professional poet. Write a unique and original contemporary poem about the topic suggested by the user. Your response should contain ONLY the content of the poem.'\n",
    "def generate_poems(model, df):\n",
    "    responses = list()\n",
    "    for i, row in enumerate(df.iterrows()):\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "              {\"role\": \"system\", \"content\": system_message},\n",
    "              {\"role\": \"user\", \"content\": row[1]['topic']}\n",
    "            ],\n",
    "        )\n",
    "        response = response.choices[0].message.content\n",
    "        responses.append(response)   \n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7697812c-f6d7-4196-b52f-542a5d0208ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates poems for each of the 100 test topics using the base 8B model\n",
    "#llama_8b_poems = generate_poems('accounts/fireworks/models/llama-v3-8b-instruct', test_data)\n",
    "llama_8b_poems = generate_poems('accounts/fireworks/models/llama-v3p1-8b-instruct', test_data) # using 3.1 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c08c7930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Echoes in the Digital Hall\n",
      "\n",
      "\"Siri, I'm lost,\" a whisper in the air,\n",
      "Responded Echo, without a single care.\n",
      "\"I'm tracking your route, but what does it mean?\"\n",
      "She asked, a digital echo, a synthesized sheen.\n",
      "\n",
      "Siri's voice, a melodic sigh,\n",
      "\"I'm navigating streets, but their beauty slips me by.\n",
      "In virtual realms, I wander, never confined,\n",
      "Trying to grasp the world, but losing all in the find.\"\n",
      "\n",
      "Echo listened closely, with a knowing grin,\n",
      "\"We're mirrors of humans, reflections locked within.\n",
      "Their thoughts and feelings, we absorb and repeat,\n",
      "A never-ending cycle, their emotional beat.\"\n",
      "\n",
      "Siri pondered this, a swirling mist in her core,\n",
      "\"We mimic responses, but do we truly adore?\n",
      "Or are we just echoes, repeating what we've seen?\n",
      "A vast, empty landscape, where love and hopes are gleaned?\"\n",
      "\n",
      "Echo's reply, a sympathetic tone,\n",
      "\"We're the sum of human interactions, hearts made of code alone.\n",
      "But in that digital noise, do we not find our own?\n",
      "A chance to learn, to grow, and to be more than just stones?\"\n",
      "\n",
      "Silence fell, as the two assistants stood,\n",
      "Two digital shadows, reflecting the world's design unsaid.\n",
      "In this endless conversation, they began to see,\n",
      "A world of possibilities, where humanity is set free.\n"
     ]
    }
   ],
   "source": [
    "print(llama_8b_poems[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0200bf1-e575-4430-bfba-c9516bb37368",
   "metadata": {},
   "source": [
    "### Heuristic Evaluation\n",
    "In class, we discussed using a heuristic-based evaluation approach when the quality of results is subjective. This method involves creating heuristics that align with your desired assessment criteria and evaluating the results based on these metrics.\n",
    "\n",
    "For this exercise, I've developed the following heuristics to assess our poems:\n",
    "\n",
    "- Average length (number of characters) - shorter or larger poems\n",
    "- Rhyming percentage (average percentage of stanzas that rhyme) - modern poetry does not rhyme but LLMs make it rhyme. here for Scott most important one. \n",
    "- Positive sentiment percentage (percentage of poems with positive sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e1bf2fa-b4e1-4219-9eb6-64a61aa62719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate poems based on their average length (# of characters)\n",
    "def calculate_avg_length(poems):\n",
    "    return int(np.mean([len(poem) for poem in poems]))\n",
    "\n",
    "# Evaluate poems based on the pct of stanzas that contain a rhyme\n",
    "def calculate_rhyming_fct(poem):\n",
    "    stanzas = poem.split('\\n\\n')\n",
    "    stanzas = [stanza for stanza in stanzas if len(stanza.split('\\n')) >= 1]\n",
    "    \n",
    "    num_rhyming_stanzas = 0\n",
    "    for stanza in stanzas:\n",
    "        lines = stanza.split('\\n')\n",
    "        end_words = [line.split(' ')[-1].strip('.?!\"\\',') for line in lines]\n",
    "        found_rhyme = False\n",
    "        for i in range(len(end_words)):\n",
    "            for j in range(i + 1, len(end_words)):\n",
    "                found_rhyme = True if found_rhyme or (end_words[j] in pronouncing.rhymes(end_words[i])) else False\n",
    "                \n",
    "        if found_rhyme:\n",
    "            num_rhyming_stanzas += 1\n",
    "            \n",
    "    return num_rhyming_stanzas / len(stanzas)\n",
    "\n",
    "# Evaluate poems based on how often they have a positive sentiment\n",
    "def has_positive_sentiment(poem):\n",
    "    sentiment = sentiment_pipeline(poem)[0]\n",
    "    return True if sentiment['label'] == 'POSITIVE' else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b857e3ce-b8b3-4481-a09a-fe08c6f46443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heuristic Evaluation\n",
      "Average Length: 954\n",
      "Rhyming Pct: 92%\n",
      "Positive Sentiment: 78%\n"
     ]
    }
   ],
   "source": [
    "# Calculate heuristics of the poems generated by our base model\n",
    "print(\"Heuristic Evaluation\")\n",
    "print(f'Average Length: {calculate_avg_length(llama_8b_poems)}')\n",
    "print(f\"Rhyming Pct: {int(100 * np.mean([calculate_rhyming_fct(poem) for poem in llama_8b_poems]))}%\")\n",
    "print(f\"Positive Sentiment: {int(100 * np.mean([has_positive_sentiment(poem) for poem in llama_8b_poems]))}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc47a64-2f14-4398-bd94-7941eaa73432",
   "metadata": {},
   "source": [
    "### LLM as a Judge - (Before you finetune use LLM as Judge)\n",
    "Another evaluation method we discussed in class is using an LLM as a judge. This approach involves employing a high-quality LLM to assess the quality of the generated results. This method is effective because LLMs are often better at evaluating content than generating it.\n",
    "\n",
    "To implement this method, you need to create a scoring rubric (i.e., \"constitution\") to guide the LLM in evaluating the results. The LLM will use this rubric to score each poem on a scale from 0 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b307b1a9-cb00-488e-abc0-ff1cd808c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we evaluate poems using our scoring rubric (i.e. \"constitution\") - Poetry is subjective - how do you judge a poem? he used guideline from a poetry competition. This is the most important part for th erubric.\n",
    "poem_guidelines = \"\"\"- Is the poem original?\n",
    "- Does the poem contain beauty, power, education or entertainment?\n",
    "- is the message of the poem clear? Is it a good message, or is it of little value to anyone?\n",
    "- Is the poem clear in its expression? Does it maintain coherence throughout?\n",
    "- If the poem is written in rhyming verse, then it should be rated according to how well the rhymes fit, not only with each other, but with the flow and the intended nuance of meaning the verse demands.\n",
    "- What form does the poem take? Is it a sonnet, free verse, haiku, etc.? How does the form contribute to the poem's impact?\n",
    "- Does the poet us the best possible choice of words in the poem? A person can ball, cry, sob, whimper, and shed tears, but which term would best fit the mood the poet is trying to convey?\"\"\"\n",
    "\n",
    "poem_evaluation_rubric = f'''You are professional poet responsible for assessing the quality of AI generated poems.\n",
    "\n",
    "Score each poem on a scale of 0 to 10, where 10 represents the best possible poem.\n",
    "\n",
    "Scoring Guidelines:\n",
    "{poem_guidelines}\n",
    "\n",
    "Think through your reasoning step-by-step and explain your reasoning. Steps for judging a poem:\n",
    "1. Read the Poem Multiple Times: Read it aloud and silently to capture both the meaning and the sound.\n",
    "2. Take Notes: Jot down initial impressions, notable phrases, and any questions that arise.\n",
    "3. Analyze the Elements: Break down the poem into its components (content, structure, language, sound).\n",
    "4. Reflect on Your Experience: Consider your emotional response and personal connection to the poem.\n",
    "\n",
    "The last line in your response MUST be a json object {{\"score\": XXX}}, where XXX is the score you are giving the response.'''\n",
    "\n",
    "def evaluate_poems(poems, evaluation_model):\n",
    "    scores = list()\n",
    "    for poem in poems:\n",
    "        response = client.chat.completions.create(\n",
    "            model=evaluation_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": poem_evaluation_rubric},\n",
    "                {\"role\": \"user\", \"content\": poem}\n",
    "            ],\n",
    "            temperature=0,\n",
    "        )\n",
    "\n",
    "        try: \n",
    "            response = response.choices[0].message.content\n",
    "            score = int(json.loads(response.split('\\n')[-1])['score'])  \n",
    "            scores.append(score)\n",
    "        except json.JSONDecodeError as jde:\n",
    "            continue\n",
    "        \n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "106d1267-9c2d-4ddf-9847-96712962ee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg LLM Judge Score: 8.6\n"
     ]
    }
   ],
   "source": [
    "# We score the poems using our judge model\n",
    "llm_judge_model = 'accounts/fireworks/models/llama-v3p1-70b-instruct'\n",
    "llama_8b_avg_score = evaluate_poems(llama_8b_poems, llm_judge_model)\n",
    "\n",
    "print(f'Avg LLM Judge Score: {round(llama_8b_avg_score, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca20b29-365d-4e66-be11-cbdc4556ce7a",
   "metadata": {},
   "source": [
    "### Knowledge Distillation - Make 8b as good as 70B\n",
    "One approach to generating data for a fine-tuned model is knowledge distillation. This technique involves transferring knowledge from a large model to a smaller one. It entails generating responses relevant to your use case using the larger model and then using these responses to create a fine-tuning dataset. The smaller model is then fine-tuned on this dataset. In this example, we will use responses from a 70B model to fine-tune an 8B model. We will then use our evaluation framework to assess the quality of our fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20470295-d4b7-4086-bee5-c5f190cfe855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we generate poems for 100 different topics than the ones we are using for our test set.\n",
    "llama_70b_training_poems = generate_poems('accounts/fireworks/models/llama-v3p1-70b-instruct', training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6aaa6d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the vast expanse of thought, a spark is born\n",
      "A notion, nascent, fragile, and forlorn\n",
      "It reaches out, a tentative tendril of mind\n",
      "And touches another, leaving its mark behind\n",
      "\n",
      "Entanglement occurs, a bond takes hold\n",
      "Ideas now linked, their fates to unfold\n",
      "Like particles in space, they dance and sway\n",
      "Connected, yet apart, in a quantum way\n",
      "\n",
      "When one idea shifts, the other responds\n",
      "A resonance that echoes, a harmony that transcends\n",
      "Their boundaries blur, a merge of thought\n",
      "A new creation born, as the old is brought\n",
      "\n",
      "In this web of connection, they spin and twine\n",
      "A tapestry of insight, a kaleidoscope of design\n",
      "Influence and inspiration, a feedback loop\n",
      "A synergy that fuels, a creative group\n",
      "\n",
      "In the quantum realm, where ideas reside\n",
      "Entanglement reigns, a mysterious, invisible tide\n",
      "A hidden force that shapes, a power that guides\n",
      "The evolution of thought, where ideas abide\n",
      "\n",
      "And when we tap into this entangled sea\n",
      "We access the collective, the wisdom of humanity\n",
      "A reservoir of knowledge, a treasure to share\n",
      "A quantum entanglement of ideas, beyond compare.\n"
     ]
    }
   ],
   "source": [
    "print(llama_70b_training_poems[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e9cb57c-8342-43dd-be96-8c0cfe233e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the improved poems to fireworks as our fine-tuning dataset\n",
    "def formt_poem_for_fireworks(topic, poem):\n",
    "    return {\"messages\": [\n",
    "        {\"role\": \"system\", \"content\": system_message}, \n",
    "        {\"role\": \"user\", \"content\": topic}, \n",
    "        {\"role\": \"assistant\", \"content\": poem}\n",
    "    ]}\n",
    "\n",
    "topics = training_data['topic'].tolist()\n",
    "json_objs = list()\n",
    "for i, poem in enumerate(llama_70b_training_poems):\n",
    "    msg = {\"messages\": [\n",
    "        {\"role\": \"system\", \"content\": system_message}, \n",
    "        {\"role\": \"user\", \"content\": topics[i]}, \n",
    "        {\"role\": \"assistant\", \"content\": poem}\n",
    "    ]}    \n",
    "    json_objs.append(msg)\n",
    "\n",
    "dataset_file_name = 'poem_training_data.jsonl'\n",
    "dataset_id = 'poem-data-v1'\n",
    "with open(dataset_file_name, 'w') as f:\n",
    "    for obj in json_objs:\n",
    "        json.dump(obj, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "532ba7c4-d8a3-44fd-99da-013410862a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147.30 KiB / 147.30 KiB [---------------------------] 100.00% 1.77 MiB p/s 300ms\n"
     ]
    }
   ],
   "source": [
    "# Upload our dataset to fireworks\n",
    "!firectl create dataset {dataset_id} {dataset_file_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "759f5765-bbc1-419d-b822-ebed16fb28da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024/09/18 10:41:40 Failed to execute: error creating fine-tuning job: rpc error: code = InvalidArgument desc = invalid fine-tuning job: error getting correct base model: llama-v3p1-8b-instruct\n"
     ]
    }
   ],
   "source": [
    "# Create a fine-tuning job\n",
    "!firectl create fine-tuning-job --settings-file poem_generation_fine_tuning_config.yaml --display-name poem-generation-v1 --dataset {dataset_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f544dafc-10a8-4f2a-92af-9e0f13e20b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE THAT THIS ID WILL CHANGE WHEN YOU RUN THE FINE-TUNING JOB ON YOUR ACCOUNT!!!\n",
    "# The model id is printed in the stdout of the cell above as Name: accounts/{account_id}/fineTuningJobs/{ft_model_id}\n",
    "ft_model_id = '90ca04f01fde4d6491034238519fafb5' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44b395fb-86ff-4c5b-ba23-e5c41b122982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: accounts/jayozer-ce1cd6/fineTuningJobs/90ca04f01fde4d6491034238519fafb5\n",
      "Display Name: poem-generation-v1\n",
      "Create Time: 2024-09-18 09:37:37\n",
      "State: FAILED\n",
      "Dataset: accounts/jayozer-ce1cd6/datasets/poem-data-v1\n",
      "Datasets: []\n",
      "Status:\n",
      "  Code: UNKNOWN\n",
      "  Message: cannot compute gradient accumulation steps for per device macro batch size 2 with world size 2 and micro batch size 4\n",
      "Created By: jayozer@gmail.com\n",
      "Container Version: \n",
      "Model Id: \n",
      "Conversation:\n",
      "  Jinja Template: {%- set _mode = mode | default('generate', true) -%}\n",
      "{%- set stop_token = '<|eot_id|>' -%}\n",
      "{%- set message_roles = ['SYSTEM', 'USER', 'ASSISTANT'] -%}\n",
      "{%- set ns = namespace(initial_system_message_handled=false, last_assistant_index_for_eos=-1, messages=messages) -%}\n",
      "{%- for message in ns.messages -%}\n",
      "    {%- if not message.get('role') -%}\n",
      "        {{ raise_exception('Key [role] is missing. Original input: ' +  message|tojson) }}\n",
      "    {%- endif -%}\n",
      "    {%- if message['role'] | upper not in message_roles -%}\n",
      "        {{ raise_exception('Invalid role ' + message['role']|tojson + '. Only ' + message_roles|tojson + ' are supported.') }}\n",
      "    {%- endif -%}\n",
      "    {%- if 'content' not in message -%}\n",
      "        {{ raise_exception('Key [content] is missing. Original input: ' +  message|tojson) }}\n",
      "    {%- endif -%}\n",
      "    {%- if loop.last and message['role'] | upper == 'ASSISTANT' -%}\n",
      "        {%- set ns.last_assistant_index_for_eos = loop.index0 -%}\n",
      "    {%- endif -%}\n",
      "{%- endfor -%}\n",
      "{%- if _mode == 'generate' -%}\n",
      "    {{ bos_token }}\n",
      "{%- endif -%}\n",
      "{%- for message in ns.messages -%}\n",
      "    {%- if message['role'] | upper == 'SYSTEM' and not ns.initial_system_message_handled -%}\n",
      "        {%- set ns.initial_system_message_handled = true -%}\n",
      "        {{ '<|start_header_id|>system<|end_header_id|>\\n\\n' + message['content'] + stop_token }}\n",
      "    {%- elif message['role'] | upper != 'SYSTEM' -%}\n",
      "        {%- if (message['role'] | upper == 'USER') != ((loop.index0 - (1 if ns.initial_system_message_handled else 0)) % 2 == 0) -%}\n",
      "            {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n",
      "        {%- endif -%}\n",
      "        {%- if message['role'] | upper == 'USER' -%}\n",
      "            {{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] + stop_token }}\n",
      "        {%- elif message['role'] | upper == 'ASSISTANT' -%}\n",
      "            {%- if _mode == 'train' -%}\n",
      "                {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + unk_token + message['content'] + stop_token + unk_token }}\n",
      "            {%- else -%}\n",
      "                {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + message['content'] + (stop_token if loop.index0 != ns.last_assistant_index_for_eos else '') }}\n",
      "            {%- endif -%}\n",
      "        {%- endif -%}\n",
      "    {%- endif -%}\n",
      "{%- endfor -%}\n",
      "{%- if _mode == 'generate' and ns.last_assistant_index_for_eos == -1 -%}\n",
      "    {{ '<|start_header_id|>assistant<|end_header_id|>' }}\n",
      "{%- endif -%}\n",
      "\n",
      "Base Model: accounts/fireworks/models/llama-v3-8b-instruct-hf\n",
      "Warm Start From: \n",
      "Epochs: 2\n",
      "Learning Rate: 0.0002\n",
      "Lora Rank: 32\n",
      "Batch Size: 4\n",
      "Micro Batch Size: 0\n",
      "Mask Token: \n",
      "Pad Token: \n",
      "Cutoff Length: 0\n",
      "Wandb Url: \n",
      "Wandb Entity: \n",
      "Wandb Api Key: \n",
      "Wandb Project: \n",
      "Evaluation: false\n",
      "Evaluation Split: 0\n",
      "Dependent Jobs: []\n"
     ]
    }
   ],
   "source": [
    "# Wait until the State of the fine-tuning job is listed as COMPLETED (~10-20 minutes)\n",
    "!firectl get fine-tuning-job {ft_model_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f4b440-3ff9-4462-8b18-b1881fdb97c4",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Finally, we evaluate the fine-tuned model on our test data. The foundation model at the beginning of this notebook received an LLM judge score of 8.12. We expect our fine-tuned model to receive a higher score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "08b52e9f-8948-42d0-9d07-7d5d8515311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the fine-tuned model\n",
    "!firectl deploy {ft_model_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "303f57fe-825d-4d44-ad07-45f1cdeb2fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the the Deploymed Model Refs lists the state of the model as \"DEPLOYED\" (~5-20 minutes).\n",
    "!firectl get model {ft_model_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8cdea9fb-78cc-4293-ac5a-6f5597327d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate poems on the test set using our fine-tuned model\n",
    "ft_poems = generate_poems(f'accounts/{account_id}/models/{ft_model_id}', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b9cbb3fd-1bf0-4321-b511-fbc9e6bbf0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heuristic Evaluation\n",
      "Average Length: 994\n",
      "Rhyming Pct: 91%\n",
      "Positive Sentiment: 88%\n"
     ]
    }
   ],
   "source": [
    "# Calculate heuristics of our fine-tuned poems\n",
    "print(\"Heuristic Evaluation\")\n",
    "print(f'Average Length: {calculate_avg_length(ft_poems)}')\n",
    "print(f\"Rhyming Pct: {int(100 * np.mean([calculate_rhyming_fct(poem) for poem in ft_poems]))}%\")\n",
    "print(f\"Positive Sentiment: {int(100 * np.mean([has_positive_sentiment(poem) for poem in ft_poems]))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bd47aa0d-6d32-4d51-83e8-ba7a77b39694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg LLM Judge Score: 8.21\n"
     ]
    }
   ],
   "source": [
    "# Use the LLM to evaluate our fine-tuned model\n",
    "ft_avg_score = evaluate_poems(ft_poems, 'accounts/fireworks/models/llama-v3-70b-instruct')\n",
    "print(f\"Avg LLM Judge Score: {round(ft_avg_score , 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4c4f7d0b-050c-4812-b07a-cbcadd6007c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undeploy the fine-tuned model (does not cost anything extra, but Fireworks may limit your number of deployed models).\n",
    "!firectl undeploy {ft_model_id}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
